# Parity Analysis and Root Cause of Field Discrepancies

## 1. Problem Summary

Following recent changes, the `debug-parity.py` script reported a significant increase in field differences (over 2000) for job `69172` when comparing the modernized output (`.set` file) against the legacy baseline. The widespread nature of these errors suggested a systemic issue rather than isolated bugs in business logic.

## 2. Investigation and Analysis

The investigation proceeded in a step-by-step manner to drill down from a high-level symptom to the precise root cause.

### Step 1: Understanding the Modernizer's Schema Generation

- **Initial Assumption:** The schemas were being extracted directly from COBOL copybooks (`.cbl` files).
- **Finding:** This was incorrect. The schemas for the modernizer pipeline are generated by a C# program (`Cnp.Cli`) which reads a Data Definition (`.dd`) file located at `MBCNTR2503.Modernizer/config/base/mblps/mblps.dd`. This `.dd` file defines the name, offset, length, and type for each field.

### Step 2: High-Level Parity Check

- The `debug-parity.py` script was run for job `69172`.
- **Finding:** The output showed massive, systemic failures across almost all field types, including packed-decimal numbers, EBCDIC text, and zoned-decimal numbers. The `Actual` values were often filled with garbage data or placeholder characters, indicating a problem with data encoding or type interpretation.

### Step 3: Byte-Level File Comparison

- A diagnostic Python script (`byte_level_compare.py`) was created to perform a direct byte-for-byte comparison of the first record from the legacy output file and the modernized output file.
- **Finding:** The files were fundamentally different from the very first few bytes. This definitively proved that the issue was not with the `debug-parity.py` script's decoding logic, but with the C# pipeline that was generating the modernized `.set` file. The output file itself was corrupted.

### Step 4: Tracing the Modernizer Pipeline

- The C# pipeline's entry point, `PipelineOrchestrator.cs`, was examined.
- **Finding:** The orchestrator pinpointed the exact stage where the output file is written: "Step 4: MB2000 Conversion". This step uses the `MB2000FieldMapper.cs` class to map the input record to the final 2000-byte output format.

### Step 5: Analyzing the Legacy Code for Ground Truth

- To confirm the true data structures, the legacy application's scripts were analyzed. The main script, `mbcntr2503.script`, identified the COBOL program `setmb2000.cbl` as the source of the MB2000 conversion logic.
- The `mb2000.cbl` copybook was then inspected.
- **Finding:** This was the "smoking gun." The copybook provided a definitive definition of the record structure, showing extensive use of:
    - **Packed-Decimal (`COMP-3`)** for most numeric fields.
    - **Zoned-Decimal (`PIC 9`)** for other numeric fields.

## 3. Root Cause

The root cause is a fundamental flaw in `MB2000FieldMapper.cs`. The C# code was incorrectly treating all numeric fields that were not explicitly "packed" as simple EBCDIC text. It lacked the specific logic required to decode **zoned-decimal** numbers.

This caused the C# pipeline to misinterpret the binary data and write out a corrupted byte stream, leading to the massive discrepancies observed by the parity checker.

## 4. Proposed Solution

The solution is to align the modernizer's data handling with the legacy system's COBOL definitions.

1.  **Enhance the C# Field Mapper:** Modify `MB2000FieldMapper.cs` to introduce a new conversion `mode` specifically for `"zoned"` data. This new logic will leverage the existing `EbcdicAsciiConverter`'s `Zoned` mode to correctly translate EBCDIC zoned-decimal numbers into their proper ASCII string representation.

2.  **Update Configuration:** After the C# code is updated, the `mb2000.overrides.json` configuration file will be updated. The `mode` for all fields identified as zoned-decimal in the `mb2000.cbl` copybook will be changed to `"zoned"`.

This approach will fix the data corruption at its source, ensuring that the modernized pipeline generates a byte-for-byte accurate `.set` file and resolving the parity issues.
